"""
æ•°æ®å¥åº·æ£€æŸ¥å™¨è¾“å…¥è¾“å‡ºæµ‹è¯•
è¯¦ç»†å±•ç¤ºè¾“å…¥ä»€ä¹ˆæ•°æ®ï¼Œè¾“å‡ºä»€ä¹ˆç»“æœ
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json

from enhanced_data_health_checker import EnhancedDataHealthChecker, HealthStatus
from data_cleaner import DataCleaner, CleaningConfig, CleaningMethod
from mock_data_generator import MockDataGenerator


def create_test_datasets():
    """åˆ›å»ºå„ç§æµ‹è¯•æ•°æ®é›†"""
    print("ğŸ”§ åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
    
    datasets = {}
    
    # 1. å®Œç¾æ•°æ® - æ— ä»»ä½•é—®é¢˜
    print("  ğŸ“Š åˆ›å»ºå®Œç¾æ•°æ®é›†...")
    generator = MockDataGenerator(base_price=150.0, volatility=0.01)
    perfect_data = generator.generate_historical_data("PERFECT", "30min", records=50)
    datasets['perfect'] = perfect_data
    print(f"     å½¢çŠ¶: {perfect_data.shape}")
    
    # 2. æœ‰ç¼ºå¤±å€¼çš„æ•°æ®
    print("  ğŸ“Š åˆ›å»ºç¼ºå¤±å€¼æ•°æ®é›†...")
    missing_data = perfect_data.copy()
    # æ·»åŠ å„ç§ç¼ºå¤±å€¼
    missing_data.iloc[5:8, 1] = np.nan  # highåˆ—ç¼ºå¤±
    missing_data.iloc[10:12, 3] = np.nan  # closeåˆ—ç¼ºå¤±
    missing_data.iloc[15, :] = np.nan  # æ•´è¡Œç¼ºå¤±
    missing_data.iloc[20:22, 4] = np.nan  # volumeåˆ—ç¼ºå¤±
    datasets['missing'] = missing_data
    print(f"     å½¢çŠ¶: {missing_data.shape}, ç¼ºå¤±å€¼: {missing_data.isnull().sum().sum()}")
    
    # 3. æœ‰å¼‚å¸¸å€¼çš„æ•°æ®
    print("  ğŸ“Š åˆ›å»ºå¼‚å¸¸å€¼æ•°æ®é›†...")
    outlier_data = perfect_data.copy()
    # æ·»åŠ å„ç§å¼‚å¸¸å€¼
    outlier_data.iloc[8, 1] = outlier_data.iloc[8, 1] * 10  # æé«˜ä»·æ ¼
    outlier_data.iloc[12, 2] = outlier_data.iloc[12, 2] * 0.1  # æä½ä»·æ ¼
    outlier_data.iloc[16, 4] = outlier_data.iloc[16, 4] * 100  # æé«˜æˆäº¤é‡
    outlier_data.iloc[25, 4] = -1000  # è´Ÿæˆäº¤é‡
    datasets['outlier'] = outlier_data
    print(f"     å½¢çŠ¶: {outlier_data.shape}")
    
    # 4. æ—¶é—´è¿ç»­æ€§é—®é¢˜æ•°æ®
    print("  ğŸ“Š åˆ›å»ºæ—¶é—´é—®é¢˜æ•°æ®é›†...")
    time_data = perfect_data.copy()
    # åˆ é™¤ä¸€äº›æ—¶é—´ç‚¹é€ æˆé—´éš™
    time_data = time_data.drop(time_data.index[10:15])  # åˆ é™¤5ä¸ªæ—¶é—´ç‚¹
    # æ·»åŠ é‡å¤æ—¶é—´æˆ³
    duplicate_row = time_data.iloc[5:6].copy()
    time_data = pd.concat([time_data, duplicate_row])
    time_data = time_data.sort_index()
    datasets['time_issues'] = time_data
    print(f"     å½¢çŠ¶: {time_data.shape}")
    
    # 5. ä»·æ ¼é€»è¾‘é”™è¯¯æ•°æ®
    print("  ğŸ“Š åˆ›å»ºä»·æ ¼é€»è¾‘é”™è¯¯æ•°æ®é›†...")
    logic_data = perfect_data.copy()
    # åˆ›å»ºOHLCé€»è¾‘é”™è¯¯
    logic_data.iloc[6, 1] = logic_data.iloc[6, 2] - 10  # high < low
    logic_data.iloc[14, 0] = 0  # open = 0
    logic_data.iloc[18, 3] = -5  # close < 0
    datasets['logic_errors'] = logic_data
    print(f"     å½¢çŠ¶: {logic_data.shape}")
    
    # 6. ç»¼åˆé—®é¢˜æ•°æ®
    print("  ğŸ“Š åˆ›å»ºç»¼åˆé—®é¢˜æ•°æ®é›†...")
    complex_data = perfect_data.copy()
    # æ·»åŠ å¤šç§é—®é¢˜
    complex_data.iloc[3:6, 1] = np.nan  # ç¼ºå¤±å€¼
    complex_data.iloc[8, 3] = complex_data.iloc[8, 3] * 20  # å¼‚å¸¸å€¼
    complex_data.iloc[12, 4] = -500  # è´Ÿæˆäº¤é‡
    complex_data.iloc[16, 1] = complex_data.iloc[16, 2] - 5  # é€»è¾‘é”™è¯¯
    complex_data = complex_data.drop(complex_data.index[20:25])  # æ—¶é—´é—´éš™
    datasets['complex'] = complex_data
    print(f"     å½¢çŠ¶: {complex_data.shape}")
    
    print(f"âœ… åˆ›å»ºäº† {len(datasets)} ä¸ªæµ‹è¯•æ•°æ®é›†")
    return datasets


def test_input_output_detailed():
    """è¯¦ç»†æµ‹è¯•è¾“å…¥è¾“å‡º"""
    print("\n" + "="*80)
    print("ğŸ§ª æ•°æ®å¥åº·æ£€æŸ¥å™¨ - è¯¦ç»†è¾“å…¥è¾“å‡ºæµ‹è¯•")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    datasets = create_test_datasets()
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = EnhancedDataHealthChecker()
    
    for data_name, df in datasets.items():
        print(f"\n{'ğŸ” æµ‹è¯•æ•°æ®é›†: ' + data_name.upper():-^70}")
        
        # æ˜¾ç¤ºè¾“å…¥æ•°æ®ä¿¡æ¯
        print(f"\nğŸ“¥ è¾“å…¥æ•°æ®:")
        print(f"   ç±»å‹: {type(df)}")
        print(f"   å½¢çŠ¶: {df.shape}")
        print(f"   åˆ—å: {list(df.columns)}")
        print(f"   ç´¢å¼•ç±»å‹: {type(df.index)}")
        print(f"   æ•°æ®ç±»å‹:\n{df.dtypes.to_string()}")
        print(f"   å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024:.2f} KB")
        
        # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
        print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬ (å‰3è¡Œ):")
        print(df.head(3).to_string())
        
        # æ˜¾ç¤ºæ•°æ®é—®é¢˜æ¦‚è§ˆ
        print(f"\nâš ï¸ æ•°æ®é—®é¢˜æ¦‚è§ˆ:")
        print(f"   ç¼ºå¤±å€¼æ€»æ•°: {df.isnull().sum().sum()}")
        print(f"   é›¶å€¼æ•°é‡: {(df == 0).sum().sum()}")
        print(f"   è´Ÿå€¼æ•°é‡: {(df < 0).sum().sum()}")
        if isinstance(df.index, pd.DatetimeIndex):
            print(f"   é‡å¤æ—¶é—´æˆ³: {df.index.duplicated().sum()}")
            time_diffs = df.index.to_series().diff().dropna()
            if len(time_diffs) > 0:
                print(f"   æ—¶é—´é—´éš”å˜åŒ–: {len(time_diffs.unique())} ç§ä¸åŒé—´éš”")
        
        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        print(f"\nğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥...")
        try:
            report = checker.comprehensive_health_check(df, clean_data=True, save_report=False)
            
            # æ˜¾ç¤ºè¾“å‡ºç»“æœ
            print(f"\nğŸ“¤ è¾“å‡ºç»“æœ:")
            print(f"   æ£€æŸ¥çŠ¶æ€: {report.status.value}")
            print(f"   é—®é¢˜æ€»æ•°: {len(report.issues)}")
            print(f"   å¤„ç†æ—¶é—´: {report.processing_time:.4f} ç§’")
            print(f"   åŸå§‹æ•°æ®: {report.original_shape}")
            print(f"   æ¸…æ´—åæ•°æ®: {report.cleaned_shape}")
            
            if report.cleaned_data is not None:
                reduction = report.original_shape[0] - report.cleaned_shape[0]
                retention = (report.cleaned_shape[0] / report.original_shape[0]) * 100
                print(f"   æ•°æ®åˆ é™¤: {reduction} è¡Œ ({100-retention:.1f}%)")
                print(f"   æ•°æ®ä¿ç•™: {retention:.1f}%")
            
            # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
            if report.issues:
                print(f"\nğŸ” å‘ç°çš„é—®é¢˜:")
                for i, issue in enumerate(report.issues[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    severity_emoji = "ğŸš¨" if issue.severity == HealthStatus.CRITICAL else "âš ï¸"
                    print(f"   {i}. {severity_emoji} [{issue.issue_type.value}] {issue.description}")
                    if issue.suggestion:
                        print(f"      ğŸ’¡ å»ºè®®: {issue.suggestion}")
                
                if len(report.issues) > 5:
                    print(f"   ... è¿˜æœ‰ {len(report.issues) - 5} ä¸ªé—®é¢˜")
            else:
                print(f"   âœ… æœªå‘ç°é—®é¢˜")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if report.statistics:
                print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
                if 'basic' in report.statistics:
                    basic = report.statistics['basic']
                    print(f"   å†…å­˜ä½¿ç”¨: {basic.get('memory_usage', 0) / 1024:.2f} KB")
                
                if 'financial' in report.statistics:
                    financial = report.statistics['financial']
                    print(f"   ä»·æ ¼æ³¢åŠ¨ç‡: {financial.get('volatility', 0):.4f}")
                    print(f"   å¹³å‡æ”¶ç›Šç‡: {financial.get('mean_return', 0):.4f}")
            
            # æ˜¾ç¤ºæ¸…æ´—åæ•°æ®æ ·æœ¬
            if report.cleaned_data is not None and not report.cleaned_data.empty:
                print(f"\nğŸ“‹ æ¸…æ´—åæ•°æ®æ ·æœ¬ (å‰3è¡Œ):")
                print(report.cleaned_data.head(3).to_string())
                
                print(f"\nâœ… æ¸…æ´—åæ•°æ®è´¨é‡:")
                print(f"   ç¼ºå¤±å€¼: {report.cleaned_data.isnull().sum().sum()}")
                print(f"   è´Ÿå€¼: {(report.cleaned_data < 0).sum().sum()}")
            
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        
        print(f"\n{'-'*70}")


def test_different_input_formats():
    """æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼"""
    print(f"\n{'ğŸ”§ æµ‹è¯•ä¸åŒè¾“å…¥æ ¼å¼':-^70}")
    
    checker = EnhancedDataHealthChecker()
    
    # 1. æµ‹è¯•ç©ºDataFrame
    print(f"\nğŸ“ æµ‹è¯•1: ç©ºDataFrame")
    empty_df = pd.DataFrame()
    print(f"   è¾“å…¥: ç©ºDataFrame")
    report = checker.comprehensive_health_check(empty_df, clean_data=False, save_report=False)
    print(f"   è¾“å‡ºçŠ¶æ€: {report.status.value}")
    print(f"   é—®é¢˜æ•°: {len(report.issues)}")
    
    # 2. æµ‹è¯•åªæœ‰ä¸€è¡Œæ•°æ®
    print(f"\nğŸ“ æµ‹è¯•2: å•è¡Œæ•°æ®")
    single_row = pd.DataFrame({
        'open': [100.0],
        'high': [105.0], 
        'low': [95.0],
        'close': [102.0],
        'volume': [1000000]
    }, index=pd.DatetimeIndex(['2024-01-01']))
    print(f"   è¾“å…¥: {single_row.shape[0]} è¡Œæ•°æ®")
    report = checker.comprehensive_health_check(single_row, clean_data=False, save_report=False)
    print(f"   è¾“å‡ºçŠ¶æ€: {report.status.value}")
    print(f"   é—®é¢˜æ•°: {len(report.issues)}")
    
    # 3. æµ‹è¯•éæ ‡å‡†åˆ—å
    print(f"\nğŸ“ æµ‹è¯•3: éæ ‡å‡†åˆ—å")
    custom_df = pd.DataFrame({
        'price_open': [100.0, 101.0],
        'price_close': [102.0, 103.0],
        'trade_volume': [1000, 1500]
    })
    print(f"   è¾“å…¥: è‡ªå®šä¹‰åˆ—å {list(custom_df.columns)}")
    report = checker.comprehensive_health_check(custom_df, clean_data=False, save_report=False)
    print(f"   è¾“å‡ºçŠ¶æ€: {report.status.value}")
    print(f"   é—®é¢˜æ•°: {len(report.issues)}")
    
    # 4. æµ‹è¯•è¶…å¤§æ•°æ®é›†æ¨¡æ‹Ÿ
    print(f"\nğŸ“ æµ‹è¯•4: å¤§æ•°æ®é›†å¤„ç†")
    generator = MockDataGenerator()
    large_df = generator.generate_historical_data("LARGE", "1min", records=5000)
    print(f"   è¾“å…¥: {large_df.shape[0]} è¡Œæ•°æ® ({large_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB)")
    start_time = datetime.now()
    report = checker.comprehensive_health_check(large_df, clean_data=True, save_report=False)
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    print(f"   è¾“å‡ºçŠ¶æ€: {report.status.value}")
    print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
    print(f"   å¤„ç†é€Ÿåº¦: {large_df.shape[0] / processing_time:.0f} è¡Œ/ç§’")


def test_cleaner_standalone():
    """æµ‹è¯•ç‹¬ç«‹çš„æ•°æ®æ¸…æ´—å™¨"""
    print(f"\n{'ğŸ§¹ æµ‹è¯•ç‹¬ç«‹æ•°æ®æ¸…æ´—å™¨':-^70}")
    
    # åˆ›å»ºæœ‰é—®é¢˜çš„æ•°æ®
    generator = MockDataGenerator()
    dirty_data = generator.generate_historical_data("DIRTY", "30min", records=100)
    
    # äººå·¥æ·»åŠ å„ç§é—®é¢˜
    dirty_data.iloc[10:15, 1] = np.nan  # ç¼ºå¤±å€¼
    dirty_data.iloc[20, 3] = dirty_data.iloc[20, 3] * 50  # å¼‚å¸¸å€¼
    dirty_data.iloc[30, 4] = -1000  # è´Ÿæˆäº¤é‡
    dirty_data.iloc[40, 0] = 0  # é›¶å¼€ç›˜ä»·
    
    print(f"\nğŸ“¥ æ¸…æ´—å™¨è¾“å…¥:")
    print(f"   æ•°æ®å½¢çŠ¶: {dirty_data.shape}")
    print(f"   ç¼ºå¤±å€¼: {dirty_data.isnull().sum().sum()}")
    print(f"   è´Ÿå€¼æ•°é‡: {(dirty_data < 0).sum().sum()}")
    print(f"   é›¶å€¼æ•°é‡: {(dirty_data == 0).sum().sum()}")
    
    # æµ‹è¯•ä¸åŒæ¸…æ´—é…ç½®
    configs = {
        "å®½æ¾æ¨¡å¼": CleaningConfig(
            missing_value_method=CleaningMethod.INTERPOLATE,
            outlier_method=CleaningMethod.MEDIAN_FILL,
            remove_invalid_ohlc=False
        ),
        "æ ‡å‡†æ¨¡å¼": CleaningConfig(
            missing_value_method=CleaningMethod.INTERPOLATE,
            outlier_method=CleaningMethod.MEDIAN_FILL,
            remove_invalid_ohlc=True
        ),
        "ä¸¥æ ¼æ¨¡å¼": CleaningConfig(
            missing_value_method=CleaningMethod.DROP,
            outlier_method=CleaningMethod.DROP,
            remove_invalid_ohlc=True
        )
    }
    
    for mode_name, config in configs.items():
        print(f"\nğŸ”§ {mode_name}:")
        cleaner = DataCleaner(config)
        cleaned_data, cleaning_log = cleaner.comprehensive_clean(dirty_data.copy())
        
        print(f"   ğŸ“¤ æ¸…æ´—å™¨è¾“å‡º:")
        print(f"     æ¸…æ´—åå½¢çŠ¶: {cleaned_data.shape}")
        print(f"     æ•°æ®ä¿ç•™ç‡: {(len(cleaned_data) / len(dirty_data)) * 100:.1f}%")
        print(f"     æ¸…æ´—æ“ä½œæ•°: {len(cleaning_log)}")
        print(f"     ç¼ºå¤±å€¼: {cleaned_data.isnull().sum().sum()}")
        print(f"     è´Ÿå€¼æ•°é‡: {(cleaned_data < 0).sum().sum()}")
        
        # æ˜¾ç¤ºæ¸…æ´—æ“ä½œ
        if cleaning_log:
            print(f"     ä¸»è¦æ“ä½œ:")
            for log_entry in cleaning_log[:3]:
                print(f"       - {log_entry['action']}: {log_entry['details']}")


def demonstrate_json_output():
    """æ¼”ç¤ºJSONè¾“å‡ºæ ¼å¼"""
    print(f"\n{'ğŸ“„ JSONè¾“å‡ºæ ¼å¼æ¼”ç¤º':-^70}")
    
    generator = MockDataGenerator()
    test_data = generator.generate_historical_data("JSON_TEST", "30min", records=20)
    
    # æ·»åŠ ä¸€äº›é—®é¢˜
    test_data.iloc[5, 1] = np.nan
    test_data.iloc[10, 3] = test_data.iloc[10, 3] * 10
    
    checker = EnhancedDataHealthChecker()
    report = checker.comprehensive_health_check(test_data, clean_data=True, save_report=False)
    
    print(f"\nğŸ“¤ JSONæŠ¥å‘Šè¾“å‡º:")
    try:
        json_output = report.to_json()
        # è§£æå¹¶ç¾åŒ–æ˜¾ç¤º
        data = json.loads(json_output)
        
        print(f"\nğŸ” æŠ¥å‘Šç»“æ„:")
        print(f"   - summary: æ‘˜è¦ä¿¡æ¯")
        print(f"   - issues: é—®é¢˜åˆ—è¡¨ ({len(data.get('issues', []))} ä¸ª)")
        print(f"   - statistics: ç»Ÿè®¡ä¿¡æ¯")
        print(f"   - timestamp: ç”Ÿæˆæ—¶é—´")
        
        print(f"\nğŸ“Š æ‘˜è¦ä¿¡æ¯:")
        summary = data.get('summary', {})
        for key, value in summary.items():
            if isinstance(value, dict):
                print(f"   {key}:")
                for sub_key, sub_value in value.items():
                    print(f"     {sub_key}: {sub_value}")
            else:
                print(f"   {key}: {value}")
        
        print(f"\nâš ï¸ é—®é¢˜ç¤ºä¾‹ (å‰2ä¸ª):")
        for i, issue in enumerate(data.get('issues', [])[:2], 1):
            print(f"   é—®é¢˜ {i}:")
            print(f"     ç±»å‹: {issue.get('issue_type')}")
            print(f"     ä¸¥é‡ç¨‹åº¦: {issue.get('severity')}")
            print(f"     æè¿°: {issue.get('description')}")
            print(f"     å»ºè®®: {issue.get('suggestion', 'æ— ')}")
        
    except Exception as e:
        print(f"âŒ JSONç”Ÿæˆå¤±è´¥: {e}")


if __name__ == "__main__":
    print("ğŸ§ª æ•°æ®å¥åº·æ£€æŸ¥å™¨ - è¾“å…¥è¾“å‡ºæµ‹è¯•")
    print("="*80)
    print("æµ‹è¯•ç›®æ ‡ï¼šè¯¦ç»†äº†è§£è¾“å…¥ä»€ä¹ˆæ•°æ®ï¼Œè¾“å‡ºä»€ä¹ˆç»“æœ")
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    test_input_output_detailed()
    test_different_input_formats()
    test_cleaner_standalone()
    demonstrate_json_output()
    
    print(f"\n" + "="*80)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("\nğŸ’¡ æ€»ç»“:")
    print("âœ… æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œå¤§å°")
    print("âœ… æä¾›è¯¦ç»†çš„é—®é¢˜è¯Šæ–­")
    print("âœ… è‡ªåŠ¨æ•°æ®æ¸…æ´—å’Œä¿®å¤")
    print("âœ… ç»“æ„åŒ–çš„JSONæŠ¥å‘Šè¾“å‡º")
    print("âœ… çµæ´»çš„é…ç½®é€‰é¡¹")
    print("âœ… é«˜æ€§èƒ½å¤„ç†èƒ½åŠ›")




