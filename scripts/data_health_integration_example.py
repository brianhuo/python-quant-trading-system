"""
æ•°æ®å¥åº·æ£€æŸ¥å™¨é›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨äº¤æ˜“ç³»ç»Ÿä¸­ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®å¥åº·æ£€æŸ¥å™¨
"""

import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, Any, Tuple

from enhanced_data_health_checker import EnhancedDataHealthChecker, HealthStatus
from data_cleaner import DataCleaner, CleaningConfig, CleaningMethod
from adaptive_data_client import AdaptiveDataClient
from enhanced_config_loader import load_config
from logger_config_integration import get_strategy_logger


class TradingDataPipeline:
    """
    äº¤æ˜“æ•°æ®å¤„ç†ç®¡é“
    é›†æˆæ•°æ®è·å–ã€å¥åº·æ£€æŸ¥ã€æ¸…æ´—å’ŒéªŒè¯
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†ç®¡é“"""
        self.config = load_config()
        self.logger = get_strategy_logger("trading_data_pipeline")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_client = AdaptiveDataClient()
        self.health_checker = EnhancedDataHealthChecker()
        
        # é…ç½®æ¸…æ´—ç­–ç•¥
        self.cleaning_config = CleaningConfig(
            missing_value_method=CleaningMethod.INTERPOLATE,
            outlier_method=CleaningMethod.MEDIAN_FILL,
            negative_volume_method=CleaningMethod.ZERO_FILL,
            remove_invalid_ohlc=True,
            standardize_frequency=True,
            target_frequency=self.config.get('DATA_TIMEFRAME', '30min')
        )
        self.data_cleaner = DataCleaner(self.cleaning_config)
        
        self.logger.info("äº¤æ˜“æ•°æ®å¤„ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def get_clean_data(self, 
                      symbol: str, 
                      timeframe: str = None, 
                      limit: int = 1000,
                      quality_threshold: HealthStatus = HealthStatus.WARNING) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        è·å–æ¸…æ´—åçš„äº¤æ˜“æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´æ¡†æ¶
            limit: æ•°æ®æ•°é‡é™åˆ¶
            quality_threshold: æ•°æ®è´¨é‡é˜ˆå€¼
            
        Returns:
            (æ¸…æ´—åæ•°æ®, å¤„ç†æŠ¥å‘Š)
        """
        timeframe = timeframe or self.config.get('DATA_TIMEFRAME', '30min')
        
        self.logger.info(f"å¼€å§‹å¤„ç† {symbol} {timeframe} æ•°æ®...")
        
        # 1. è·å–åŸå§‹æ•°æ®
        raw_data = self.data_client.get_historical_data(
            symbol=symbol, 
            timeframe=timeframe, 
            limit=limit
        )
        
        if raw_data.empty:
            self.logger.error(f"æ— æ³•è·å– {symbol} çš„æ•°æ®")
            return pd.DataFrame(), {"error": "No data available"}
        
        self.logger.info(f"è·å–åˆ° {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        
        # 2. æ‰§è¡Œå¥åº·æ£€æŸ¥
        health_report = self.health_checker.comprehensive_health_check(
            raw_data, 
            clean_data=False, 
            save_report=False
        )
        
        # 3. è¯„ä¼°æ•°æ®è´¨é‡
        if health_report.status == HealthStatus.FAILED:
            self.logger.error("æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•å¤„ç†")
            return pd.DataFrame(), {
                "error": "Data quality check failed",
                "health_report": health_report.get_summary()
            }
        
        # 4. æ•°æ®æ¸…æ´—
        cleaned_data, cleaning_log = self.data_cleaner.comprehensive_clean(raw_data)
        
        # 5. æ¸…æ´—åéªŒè¯
        final_health_report = self.health_checker.comprehensive_health_check(
            cleaned_data, 
            clean_data=False, 
            save_report=False
        )
        
        # 6. è´¨é‡é—¨æ§
        if final_health_report.status.value > quality_threshold.value:
            self.logger.warning(f"æ¸…æ´—åæ•°æ®è´¨é‡ä»ä¸è¾¾æ ‡: {final_health_report.status.value}")
        
        # 7. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        processing_report = {
            "symbol": symbol,
            "timeframe": timeframe,
            "original_shape": raw_data.shape,
            "cleaned_shape": cleaned_data.shape,
            "data_reduction": {
                "rows_removed": raw_data.shape[0] - cleaned_data.shape[0],
                "removal_percentage": ((raw_data.shape[0] - cleaned_data.shape[0]) / raw_data.shape[0]) * 100
            },
            "original_health": health_report.get_summary(),
            "final_health": final_health_report.get_summary(),
            "cleaning_summary": self.data_cleaner.get_cleaning_summary(),
            "quality_passed": final_health_report.status.value <= quality_threshold.value
        }
        
        self.logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: {raw_data.shape} â†’ {cleaned_data.shape}")
        
        return cleaned_data, processing_report
    
    def batch_process_symbols(self, 
                            symbols: list, 
                            timeframe: str = None, 
                            limit: int = 1000) -> Dict[str, Tuple[pd.DataFrame, Dict]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªè‚¡ç¥¨çš„æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            timeframe: æ—¶é—´æ¡†æ¶
            limit: æ•°æ®æ•°é‡é™åˆ¶
            
        Returns:
            {symbol: (cleaned_data, report)}
        """
        results = {}
        
        for symbol in symbols:
            try:
                cleaned_data, report = self.get_clean_data(symbol, timeframe, limit)
                results[symbol] = (cleaned_data, report)
                
                # ç®€è¦æ—¥å¿—
                if not cleaned_data.empty:
                    status = "âœ… æˆåŠŸ" if report["quality_passed"] else "âš ï¸ è­¦å‘Š"
                    self.logger.info(f"{symbol}: {status} - {cleaned_data.shape[0]} è¡Œæ•°æ®")
                else:
                    self.logger.error(f"{symbol}: âŒ å¤±è´¥")
                    
            except Exception as e:
                self.logger.error(f"{symbol} å¤„ç†å¤±è´¥: {e}")
                results[symbol] = (pd.DataFrame(), {"error": str(e)})
        
        return results
    
    def get_data_quality_summary(self, processing_results: Dict[str, Tuple[pd.DataFrame, Dict]]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®è´¨é‡æ‘˜è¦æŠ¥å‘Š
        
        Args:
            processing_results: æ‰¹é‡å¤„ç†ç»“æœ
            
        Returns:
            è´¨é‡æ‘˜è¦æŠ¥å‘Š
        """
        total_symbols = len(processing_results)
        successful_symbols = 0
        total_original_rows = 0
        total_cleaned_rows = 0
        quality_issues = []
        
        for symbol, (data, report) in processing_results.items():
            if not data.empty and "error" not in report:
                successful_symbols += 1
                total_original_rows += report["original_shape"][0]
                total_cleaned_rows += report["cleaned_shape"][0]
                
                # æ”¶é›†è´¨é‡é—®é¢˜
                if not report["quality_passed"]:
                    quality_issues.append({
                        "symbol": symbol,
                        "issues": report["final_health"]["total_issues"],
                        "status": report["final_health"]["overall_status"]
                    })
        
        summary = {
            "processing_timestamp": datetime.now().isoformat(),
            "total_symbols": total_symbols,
            "successful_symbols": successful_symbols,
            "success_rate": (successful_symbols / total_symbols) * 100 if total_symbols > 0 else 0,
            "data_statistics": {
                "total_original_rows": total_original_rows,
                "total_cleaned_rows": total_cleaned_rows,
                "total_rows_removed": total_original_rows - total_cleaned_rows,
                "overall_data_retention": (total_cleaned_rows / total_original_rows) * 100 if total_original_rows > 0 else 0
            },
            "quality_issues": quality_issues,
            "symbols_with_issues": len(quality_issues)
        }
        
        return summary


def demo_trading_data_pipeline():
    """æ¼”ç¤ºäº¤æ˜“æ•°æ®å¤„ç†ç®¡é“"""
    print("ğŸ”„ äº¤æ˜“æ•°æ®å¤„ç†ç®¡é“æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ•°æ®å¤„ç†ç®¡é“
    pipeline = TradingDataPipeline()
    
    # å•ä¸ªè‚¡ç¥¨å¤„ç†ç¤ºä¾‹
    print("\nğŸ“Š å•ä¸ªè‚¡ç¥¨æ•°æ®å¤„ç†æ¼”ç¤º...")
    cleaned_data, report = pipeline.get_clean_data("AAPL", "30min", 100)
    
    if not cleaned_data.empty:
        print(f"âœ… AAPL æ•°æ®å¤„ç†æˆåŠŸ:")
        print(f"   åŸå§‹æ•°æ®: {report['original_shape'][0]} è¡Œ")
        print(f"   æ¸…æ´—å: {report['cleaned_shape'][0]} è¡Œ")
        print(f"   æ•°æ®ä¿ç•™ç‡: {100 - report['data_reduction']['removal_percentage']:.1f}%")
        print(f"   è´¨é‡çŠ¶æ€: {report['final_health']['overall_status']}")
        
        print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ:")
        print(cleaned_data.head())
    else:
        print(f"âŒ AAPL æ•°æ®å¤„ç†å¤±è´¥")
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹
    print(f"\nğŸ“ˆ æ‰¹é‡å¤„ç†æ¼”ç¤º...")
    symbols = ["AAPL", "GOOGL", "MSFT"]
    batch_results = pipeline.batch_process_symbols(symbols, "30min", 50)
    
    # è´¨é‡æ‘˜è¦
    quality_summary = pipeline.get_data_quality_summary(batch_results)
    
    print(f"\nğŸ“Š æ‰¹é‡å¤„ç†è´¨é‡æ‘˜è¦:")
    print(f"   å¤„ç†è‚¡ç¥¨æ•°: {quality_summary['total_symbols']}")
    print(f"   æˆåŠŸç‡: {quality_summary['success_rate']:.1f}%")
    print(f"   æ€»æ•°æ®ä¿ç•™ç‡: {quality_summary['data_statistics']['overall_data_retention']:.1f}%")
    print(f"   æœ‰è´¨é‡é—®é¢˜çš„è‚¡ç¥¨: {quality_summary['symbols_with_issues']}")
    
    if quality_summary['quality_issues']:
        print(f"\nâš ï¸ è´¨é‡é—®é¢˜è¯¦æƒ…:")
        for issue in quality_summary['quality_issues']:
            print(f"   {issue['symbol']}: {issue['issues']} ä¸ªé—®é¢˜ ({issue['status']})")


def demo_custom_quality_pipeline():
    """æ¼”ç¤ºè‡ªå®šä¹‰è´¨é‡ç®¡é“"""
    print("\nğŸ› ï¸ è‡ªå®šä¹‰è´¨é‡ç®¡é“æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºè‡ªå®šä¹‰æ¸…æ´—é…ç½®
    strict_config = CleaningConfig(
        missing_value_method=CleaningMethod.DROP,  # ä¸¥æ ¼æ¨¡å¼ï¼šåˆ é™¤ç¼ºå¤±å€¼
        outlier_method=CleaningMethod.DROP,        # ä¸¥æ ¼æ¨¡å¼ï¼šåˆ é™¤å¼‚å¸¸å€¼
        remove_invalid_ohlc=True,
        standardize_frequency=True,
        target_frequency="1h"  # æ ‡å‡†åŒ–ä¸º1å°æ—¶
    )
    
    # åˆ›å»ºè‡ªå®šä¹‰ç®¡é“
    pipeline = TradingDataPipeline()
    pipeline.data_cleaner = DataCleaner(strict_config)
    
    print("ğŸ”§ ä½¿ç”¨ä¸¥æ ¼æ¸…æ´—æ¨¡å¼...")
    cleaned_data, report = pipeline.get_clean_data("AAPL", "30min", 100)
    
    if not cleaned_data.empty:
        print(f"âœ… ä¸¥æ ¼æ¨¡å¼å¤„ç†ç»“æœ:")
        print(f"   æ•°æ®ä¿ç•™ç‡: {100 - report['data_reduction']['removal_percentage']:.1f}%")
        print(f"   æœ€ç»ˆè´¨é‡: {report['final_health']['overall_status']}")
        print(f"   æ¸…æ´—æ“ä½œ: {report['cleaning_summary']['total_actions']} æ¬¡")
    
    print("\nâœ¨ è‡ªå®šä¹‰ç®¡é“æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_trading_data_pipeline()
    demo_custom_quality_pipeline()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å¢å¼ºç‰ˆæ•°æ®å¥åº·æ£€æŸ¥å™¨é›†æˆæ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä¸»è¦ç‰¹æ€§:")
    print("âœ… è‡ªåŠ¨æ•°æ®è·å–ä¸æ¸…æ´—")
    print("âœ… å¤šçº§è´¨é‡æ£€æŸ¥ä¸éªŒè¯") 
    print("âœ… æ‰¹é‡å¤„ç†ä¸è´¨é‡ç›‘æ§")
    print("âœ… å¯é…ç½®çš„æ¸…æ´—ç­–ç•¥")
    print("âœ… è¯¦ç»†çš„å¤„ç†æŠ¥å‘Š")
    print("âœ… ä¼ä¸šçº§é”™è¯¯å¤„ç†")
    print("\nğŸš€ ç°åœ¨æ‚¨æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„æ•°æ®è´¨é‡ç®¡ç†è§£å†³æ–¹æ¡ˆï¼")

