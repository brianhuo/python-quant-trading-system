"""
äº¤æ˜“ç³»ç»Ÿæ•°æ®ç®¡é“ - ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®è·å–å’Œæ¸…æ´—
æ•´åˆå››ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šé…ç½®åŠ è½½å™¨ + æ—¥å¿—ç³»ç»Ÿ + ç»Ÿä¸€æ•°æ®å®¢æˆ·ç«¯ + æ•°æ®å¥åº·æ£€æŸ¥å™¨
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass
import json
from pathlib import Path

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from enhanced_config_loader import load_config
from logger_config_integration import get_strategy_logger
from adaptive_data_client import AdaptiveDataClient
from enhanced_data_health_checker import EnhancedDataHealthChecker, HealthStatus
from data_cleaner import DataCleaner, CleaningConfig, CleaningMethod


@dataclass
class DataPipelineConfig:
    """æ•°æ®ç®¡é“é…ç½®"""
    # æ•°æ®æºé…ç½®
    symbol: str = "AAPL"
    timeframe: str = "30min"
    limit: int = 1000
    
    # è´¨é‡æ§åˆ¶é…ç½®
    quality_threshold: HealthStatus = HealthStatus.WARNING
    enable_cleaning: bool = True
    cleaning_mode: str = "standard"  # conservative, standard, strict
    
    # ç¼“å­˜é…ç½®
    enable_cache: bool = True
    cache_expiry_hours: int = 1
    
    # å®æ—¶æ•°æ®é…ç½®
    enable_realtime: bool = False
    realtime_callback: Optional[Callable] = None


class TradingDataPipeline:
    """
    äº¤æ˜“ç³»ç»Ÿæ•°æ®ç®¡é“
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. é…ç½®ç®¡ç† - ç»Ÿä¸€é…ç½®åŠ è½½å’Œç®¡ç†
    2. æ—¥å¿—ç³»ç»Ÿ - ä¼ä¸šçº§æ—¥å¿—è®°å½•
    3. æ•°æ®è·å– - å†å²å’Œå®æ—¶æ•°æ®ç»Ÿä¸€æ¥å£
    4. æ•°æ®æ¸…æ´— - æ™ºèƒ½æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—
    """
    
    def __init__(self, 
                 config_path: str = None,
                 environment: str = "development",
                 pipeline_config: DataPipelineConfig = None):
        """
        åˆå§‹åŒ–äº¤æ˜“æ•°æ®ç®¡é“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            environment: è¿è¡Œç¯å¢ƒ (development/testing/production)
            pipeline_config: æ•°æ®ç®¡é“ä¸“ç”¨é…ç½®
        """
        self.environment = environment
        self.pipeline_config = pipeline_config or DataPipelineConfig()
        
        # 1. åˆå§‹åŒ–é…ç½®ç³»ç»Ÿ
        self.config = load_config(environment=environment)
        
        # 2. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = get_strategy_logger("trading_data_pipeline")
        self.logger.info(f"äº¤æ˜“æ•°æ®ç®¡é“åˆå§‹åŒ– - ç¯å¢ƒ: {environment}")
        
        # 3. åˆå§‹åŒ–æ•°æ®å®¢æˆ·ç«¯
        self.data_client = AdaptiveDataClient(self.config, self.logger)
        
        # 4. åˆå§‹åŒ–æ•°æ®å¥åº·æ£€æŸ¥å™¨
        self.health_checker = EnhancedDataHealthChecker(self.config, self.logger)
        
        # 5. åˆå§‹åŒ–æ•°æ®æ¸…æ´—å™¨
        self.data_cleaner = self._create_data_cleaner()
        
        # çŠ¶æ€ç®¡ç†
        self.pipeline_stats = {
            "initialized_at": datetime.now(),
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_rows_processed": 0,
            "total_rows_cleaned": 0,
            "average_processing_time": 0.0
        }
        
        self.logger.info("äº¤æ˜“æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def _create_data_cleaner(self) -> DataCleaner:
        """æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®æ¸…æ´—å™¨"""
        cleaning_configs = {
            "conservative": CleaningConfig(
                missing_value_method=CleaningMethod.INTERPOLATE,
                outlier_method=CleaningMethod.MEDIAN_FILL,
                negative_volume_method=CleaningMethod.ZERO_FILL,
                remove_invalid_ohlc=False,
                standardize_frequency=True,
                target_frequency=self.pipeline_config.timeframe
            ),
            "standard": CleaningConfig(
                missing_value_method=CleaningMethod.INTERPOLATE,
                outlier_method=CleaningMethod.MEDIAN_FILL,
                negative_volume_method=CleaningMethod.ZERO_FILL,
                remove_invalid_ohlc=True,
                standardize_frequency=True,
                target_frequency=self.pipeline_config.timeframe
            ),
            "strict": CleaningConfig(
                missing_value_method=CleaningMethod.DROP,
                outlier_method=CleaningMethod.DROP,
                negative_volume_method=CleaningMethod.DROP,
                remove_invalid_ohlc=True,
                standardize_frequency=True,
                target_frequency=self.pipeline_config.timeframe
            )
        }
        
        config = cleaning_configs.get(self.pipeline_config.cleaning_mode, cleaning_configs["standard"])
        return DataCleaner(config, self.logger)
    
    def get_clean_data(self, 
                      symbol: str = None,
                      timeframe: str = None,
                      limit: int = None,
                      force_refresh: bool = False) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        è·å–æ¸…æ´—åçš„æ•°æ® - æ ¸å¿ƒæ–¹æ³•
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            timeframe: æ—¶é—´æ¡†æ¶
            limit: æ•°æ®æ•°é‡é™åˆ¶
            force_refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            (æ¸…æ´—åæ•°æ®, å¤„ç†æŠ¥å‘Š)
        """
        # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        symbol = symbol or self.pipeline_config.symbol
        timeframe = timeframe or self.pipeline_config.timeframe
        limit = limit or self.pipeline_config.limit
        
        start_time = datetime.now()
        self.pipeline_stats["total_requests"] += 1
        
        try:
            self.logger.info(f"å¼€å§‹è·å–æ•°æ®: {symbol} {timeframe} limit={limit}")
            
            # æ­¥éª¤1: è·å–åŸå§‹æ•°æ®
            raw_data = self.data_client.get_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                limit=limit
            )
            
            if raw_data.empty:
                self.logger.error(f"æ— æ³•è·å– {symbol} çš„æ•°æ®")
                self.pipeline_stats["failed_requests"] += 1
                return pd.DataFrame(), {
                    "status": "failed",
                    "error": "No data available",
                    "symbol": symbol,
                    "timeframe": timeframe
                }
            
            self.logger.info(f"è·å–åŸå§‹æ•°æ®: {raw_data.shape[0]} è¡Œ")
            original_rows = raw_data.shape[0]
            
            # æ­¥éª¤2: æ•°æ®è´¨é‡æ£€æŸ¥
            health_report = self.health_checker.comprehensive_health_check(
                raw_data,
                clean_data=False,
                save_report=False
            )
            
            self.logger.info(f"æ•°æ®å¥åº·æ£€æŸ¥å®Œæˆ: {health_report.status.value} - {len(health_report.issues)} ä¸ªé—®é¢˜")
            
            # æ­¥éª¤3: è´¨é‡é—¨æ§
            if health_report.status == HealthStatus.FAILED:
                self.logger.error("æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥")
                self.pipeline_stats["failed_requests"] += 1
                return pd.DataFrame(), {
                    "status": "failed",
                    "error": "Data quality check failed",
                    "health_report": health_report.get_summary(),
                    "symbol": symbol,
                    "timeframe": timeframe
                }
            
            # æ­¥éª¤4: æ•°æ®æ¸…æ´— (å¦‚æœå¯ç”¨)
            if self.pipeline_config.enable_cleaning:
                cleaned_data, cleaning_log = self.data_cleaner.comprehensive_clean(raw_data)
                self.logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {raw_data.shape[0]} â†’ {cleaned_data.shape[0]} è¡Œ")
            else:
                cleaned_data = raw_data
                cleaning_log = []
                self.logger.info("è·³è¿‡æ•°æ®æ¸…æ´—")
            
            # æ­¥éª¤5: æ¸…æ´—åè´¨é‡éªŒè¯
            final_health_report = self.health_checker.comprehensive_health_check(
                cleaned_data,
                clean_data=False,
                save_report=False
            )
            
            # æ­¥éª¤6: æœ€ç»ˆè´¨é‡é—¨æ§
            if final_health_report.status.value > self.pipeline_config.quality_threshold.value:
                self.logger.warning(f"æ¸…æ´—åæ•°æ®è´¨é‡ä»ä¸è¾¾æ ‡: {final_health_report.status.value}")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = (datetime.now() - start_time).total_seconds()
            self.pipeline_stats["successful_requests"] += 1
            self.pipeline_stats["total_rows_processed"] += original_rows
            self.pipeline_stats["total_rows_cleaned"] += cleaned_data.shape[0]
            
            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            total_successful = self.pipeline_stats["successful_requests"]
            current_avg = self.pipeline_stats["average_processing_time"]
            self.pipeline_stats["average_processing_time"] = (
                (current_avg * (total_successful - 1) + processing_time) / total_successful
            )
            
            # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            processing_report = {
                "status": "success",
                "symbol": symbol,
                "timeframe": timeframe,
                "processing_time_seconds": processing_time,
                "data_flow": {
                    "original_rows": original_rows,
                    "cleaned_rows": cleaned_data.shape[0],
                    "rows_removed": original_rows - cleaned_data.shape[0],
                    "removal_percentage": ((original_rows - cleaned_data.shape[0]) / original_rows) * 100
                },
                "quality_assessment": {
                    "original_status": health_report.status.value,
                    "original_issues": len(health_report.issues),
                    "final_status": final_health_report.status.value,
                    "final_issues": len(final_health_report.issues),
                    "quality_improved": len(health_report.issues) > len(final_health_report.issues)
                },
                "cleaning_summary": {
                    "enabled": self.pipeline_config.enable_cleaning,
                    "mode": self.pipeline_config.cleaning_mode,
                    "operations": len(cleaning_log),
                    "details": cleaning_log[:5] if cleaning_log else []  # å‰5ä¸ªæ“ä½œ
                },
                "data_characteristics": {
                    "timespan": {
                        "start": cleaned_data.index[0].isoformat() if not cleaned_data.empty else None,
                        "end": cleaned_data.index[-1].isoformat() if not cleaned_data.empty else None,
                        "duration": str(cleaned_data.index[-1] - cleaned_data.index[0]) if len(cleaned_data) > 1 else None
                    },
                    "statistics": final_health_report.statistics.get("financial", {})
                }
            }
            
            self.logger.info(f"æ•°æ®ç®¡é“å¤„ç†å®Œæˆ: {processing_report['status']}")
            return cleaned_data, processing_report
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            self.pipeline_stats["failed_requests"] += 1
            
            error_report = {
                "status": "error",
                "error_message": str(e),
                "error_type": type(e).__name__,
                "symbol": symbol,
                "timeframe": timeframe,
                "processing_time_seconds": processing_time
            }
            
            self.logger.error(f"æ•°æ®ç®¡é“å¤„ç†å¤±è´¥: {e}")
            return pd.DataFrame(), error_report
    
    def get_realtime_data(self, 
                         symbol: str = None,
                         callback: Callable = None) -> bool:
        """
        å¯åŠ¨å®æ—¶æ•°æ®è®¢é˜…
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            callback: æ•°æ®å›è°ƒå‡½æ•°
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯åŠ¨è®¢é˜…
        """
        symbol = symbol or self.pipeline_config.symbol
        callback = callback or self.pipeline_config.realtime_callback
        
        if not self.pipeline_config.enable_realtime:
            self.logger.warning("å®æ—¶æ•°æ®åŠŸèƒ½æœªå¯ç”¨")
            return False
        
        if not callback:
            self.logger.error("å®æ—¶æ•°æ®éœ€è¦æä¾›å›è°ƒå‡½æ•°")
            return False
        
        try:
            # åˆ›å»ºåŒ…è£…å›è°ƒï¼ŒåŠ å…¥æ•°æ®è´¨é‡æ£€æŸ¥
            def quality_checked_callback(market_data):
                try:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ å®æ—¶æ•°æ®çš„è´¨é‡æ£€æŸ¥é€»è¾‘
                    self.logger.debug(f"å®æ—¶æ•°æ®: {market_data.symbol} = ${market_data.price}")
                    callback(market_data)
                except Exception as e:
                    self.logger.error(f"å®æ—¶æ•°æ®å›è°ƒé”™è¯¯: {e}")
            
            success = self.data_client.subscribe_realtime(symbol, quality_checked_callback)
            
            if success:
                self.logger.info(f"å®æ—¶æ•°æ®è®¢é˜…æˆåŠŸ: {symbol}")
            else:
                self.logger.error(f"å®æ—¶æ•°æ®è®¢é˜…å¤±è´¥: {symbol}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"å®æ—¶æ•°æ®è®¢é˜…å¼‚å¸¸: {e}")
            return False
    
    def batch_process_symbols(self, 
                            symbols: List[str],
                            timeframe: str = None,
                            limit: int = None) -> Dict[str, Tuple[pd.DataFrame, Dict]]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªäº¤æ˜“å¯¹
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            timeframe: æ—¶é—´æ¡†æ¶
            limit: æ•°æ®æ•°é‡é™åˆ¶
            
        Returns:
            {symbol: (cleaned_data, report)}
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(symbols)} ä¸ªäº¤æ˜“å¯¹")
        
        results = {}
        successful_count = 0
        
        for i, symbol in enumerate(symbols, 1):
            self.logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(symbols)} - {symbol}")
            
            try:
                cleaned_data, report = self.get_clean_data(symbol, timeframe, limit)
                results[symbol] = (cleaned_data, report)
                
                if report["status"] == "success":
                    successful_count += 1
                    self.logger.info(f"{symbol}: âœ… æˆåŠŸ - {cleaned_data.shape[0]} è¡Œ")
                else:
                    self.logger.error(f"{symbol}: âŒ å¤±è´¥ - {report.get('error', 'Unknown error')}")
                    
            except Exception as e:
                self.logger.error(f"{symbol}: âŒ å¼‚å¸¸ - {e}")
                results[symbol] = (pd.DataFrame(), {
                    "status": "error",
                    "error_message": str(e),
                    "symbol": symbol
                })
        
        self.logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {successful_count}/{len(symbols)} æˆåŠŸ")
        return results
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """è·å–ç®¡é“çŠ¶æ€"""
        return {
            "pipeline_info": {
                "environment": self.environment,
                "initialized_at": self.pipeline_stats["initialized_at"].isoformat(),
                "uptime_seconds": (datetime.now() - self.pipeline_stats["initialized_at"]).total_seconds()
            },
            "configuration": {
                "symbol": self.pipeline_config.symbol,
                "timeframe": self.pipeline_config.timeframe,
                "cleaning_mode": self.pipeline_config.cleaning_mode,
                "quality_threshold": self.pipeline_config.quality_threshold.value,
                "cache_enabled": self.pipeline_config.enable_cache,
                "realtime_enabled": self.pipeline_config.enable_realtime
            },
            "statistics": self.pipeline_stats,
            "performance": {
                "success_rate": (self.pipeline_stats["successful_requests"] / max(self.pipeline_stats["total_requests"], 1)) * 100,
                "data_retention_rate": (self.pipeline_stats["total_rows_cleaned"] / max(self.pipeline_stats["total_rows_processed"], 1)) * 100,
                "average_processing_time": self.pipeline_stats["average_processing_time"]
            },
            "component_status": {
                "config_loader": "active",
                "logger": "active", 
                "data_client": self.data_client.get_status(),
                "health_checker": "active",
                "data_cleaner": "active"
            }
        }
    
    def save_pipeline_report(self, file_path: str = None) -> str:
        """ä¿å­˜ç®¡é“çŠ¶æ€æŠ¥å‘Š"""
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"logs/trading_pipeline_report_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        status = self.get_pipeline_status()
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(status, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ç®¡é“çŠ¶æ€æŠ¥å‘Šå·²ä¿å­˜: {file_path}")
        return file_path
    
    def shutdown(self):
        """ä¼˜é›…å…³é—­ç®¡é“"""
        self.logger.info("å¼€å§‹å…³é—­äº¤æ˜“æ•°æ®ç®¡é“...")
        
        try:
            # å…³é—­æ•°æ®å®¢æˆ·ç«¯
            if hasattr(self.data_client, 'close'):
                self.data_client.close()
            
            # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
            self.save_pipeline_report()
            
            self.logger.info("äº¤æ˜“æ•°æ®ç®¡é“å·²ä¼˜é›…å…³é—­")
            
        except Exception as e:
            self.logger.error(f"ç®¡é“å…³é—­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


def create_default_pipeline() -> TradingDataPipeline:
    """åˆ›å»ºé»˜è®¤é…ç½®çš„æ•°æ®ç®¡é“"""
    default_config = DataPipelineConfig(
        symbol="AAPL",
        timeframe="30min",
        limit=1000,
        quality_threshold=HealthStatus.WARNING,
        enable_cleaning=True,
        cleaning_mode="standard",
        enable_cache=True,
        enable_realtime=False
    )
    
    return TradingDataPipeline(
        environment="development",
        pipeline_config=default_config
    )


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸš€ äº¤æ˜“ç³»ç»Ÿæ•°æ®ç®¡é“æ¼”ç¤º")
    print("=" * 70)
    
    # åˆ›å»ºæ•°æ®ç®¡é“
    pipeline = create_default_pipeline()
    
    # è·å–å•ä¸ªè‚¡ç¥¨æ•°æ®
    print("\nğŸ“Š è·å–å•ä¸ªè‚¡ç¥¨æ•°æ®...")
    data, report = pipeline.get_clean_data("AAPL", "30min", 100)
    
    if not data.empty:
        print(f"âœ… æˆåŠŸè·å–æ•°æ®: {data.shape[0]} è¡Œ")
        print(f"ğŸ“ˆ æ•°æ®æ—¶é—´èŒƒå›´: {data.index[0]} åˆ° {data.index[-1]}")
        print(f"ğŸ” å¤„ç†çŠ¶æ€: {report['status']}")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {report['processing_time_seconds']:.4f} ç§’")
        
        print(f"\nğŸ“‹ æ•°æ®é¢„è§ˆ:")
        print(data.head(3).to_string())
    else:
        print(f"âŒ æ•°æ®è·å–å¤±è´¥: {report.get('error', 'Unknown error')}")
    
    # æ‰¹é‡å¤„ç†ç¤ºä¾‹
    print(f"\nğŸ“ˆ æ‰¹é‡å¤„ç†ç¤ºä¾‹...")
    symbols = ["AAPL", "GOOGL"]
    batch_results = pipeline.batch_process_symbols(symbols, "30min", 50)
    
    for symbol, (data, report) in batch_results.items():
        status = "âœ…" if report["status"] == "success" else "âŒ"
        rows = data.shape[0] if not data.empty else 0
        print(f"   {symbol}: {status} {rows} è¡Œ")
    
    # æ˜¾ç¤ºç®¡é“çŠ¶æ€
    print(f"\nğŸ“Š ç®¡é“çŠ¶æ€:")
    status = pipeline.get_pipeline_status()
    print(f"   æˆåŠŸç‡: {status['performance']['success_rate']:.1f}%")
    print(f"   æ•°æ®ä¿ç•™ç‡: {status['performance']['data_retention_rate']:.1f}%")
    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {status['performance']['average_processing_time']:.4f} ç§’")
    
    # ä¼˜é›…å…³é—­
    pipeline.shutdown()
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼æ‚¨ç°åœ¨æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„äº¤æ˜“ç³»ç»Ÿæ•°æ®è·å–å’Œæ¸…æ´—æ¨¡å—ï¼")
